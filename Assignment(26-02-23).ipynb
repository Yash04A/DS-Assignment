{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e9426e1-bd3d-46f5-9822-1e8a5b72e038",
   "metadata": {},
   "source": [
    "# Assignment 21/02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b799f277-c55c-4004-b9d5-0ceb39e7deae",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b162a7b-22a2-4a5d-8291-dcf1d37d4933",
   "metadata": {},
   "source": [
    "Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. There are many different ways to perform web scraping to obtain data from websites. These include using online services, particular API’s or even creating your code for web scraping from scratch. Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured format. This is the best option, but there are other sites that don’t allow users to access large amounts of data in a structured form or they are simply not that technologically advanced. In that situation, it’s best to use Web Scraping to scrape the website for data.\n",
    "\n",
    "Web Scraping is used for :\n",
    "1. Price Monitoring\n",
    "2. Market Research\n",
    "3. News Monitoring\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a59a18-1e4f-4815-82c2-70e8e6ef0ebd",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda79b3-8dfd-4b86-bfc9-32b0f02c8917",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e65091a-e90f-4038-9b70-bdd144892c97",
   "metadata": {},
   "source": [
    "Methods can be used for Web Scraping :\n",
    "1. Manual Web Scraping\n",
    "2. Using a web scraping service\n",
    "3. Designing a scraper using a programming language\n",
    "4. Using a web scraping API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c78e8-5ffd-4cd3-95e1-28a46e58b564",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e362643e-2ec8-49e2-935c-8964b657c749",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3b533f-89c1-4d88-aa3b-ad872021485a",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89875073-1ea6-450f-b19d-4d5e6098f6bb",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6d9c3-698b-49c8-bc13-240ee27467ec",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d28d4-38cc-4b89-a948-7efcb2215888",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c063bc7-75bf-4a3c-8674-759654de5ad4",
   "metadata": {},
   "source": [
    "We use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape. The first line imports the Flask class and the render_template method from the flask library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1dc756-e9eb-482e-82a3-79e82b180650",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444ab4c0-94a0-406b-96ed-3abd54631559",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba773675-43b1-48a9-aa83-6a33a7f1e48e",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2d83ee-7e8f-4ecc-b84b-968ebd5947d0",
   "metadata": {},
   "source": [
    "AWS sevices used in the project :\n",
    "1. Data Pipeline\n",
    "\n",
    "    AWS Data Pipeline is a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premises data sources, at specified intervals. It helps you easily create complex data processing workloads that are fault tolerant, repeatable, and highly available.\n",
    "    \n",
    "2. AWS Beanstalk\n",
    "\n",
    "    AWS Elastic Beanstalk automates the details of capacity provisioning, load balancing, auto scaling, and application deployment, creating an environment that runs a version of your application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
